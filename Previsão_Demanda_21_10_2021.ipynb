{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Previsão_Demanda_21_10_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtanzaBxPrWy5G09LltNxn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaquesZanon/JOB/blob/main/Previs%C3%A3o_Demanda_21_10_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0VuGHxK4cNu"
      },
      "source": [
        "!pip install utils\n",
        "!pip install pmdarima # para autoARIMA\n",
        "!pip install pyEDM # Para Empirical Dynamic Modeling\n",
        "!pip install croston # Para modelo croston\n",
        "!pip install prophet # Para modelo do Facebook prophet\n",
        "!pip install tqdm\n",
        "!pip install darts\n",
        "!pip install 'u8darts[all]'\n",
        "!pip install skedm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKvNmq0c6JEz"
      },
      "source": [
        "### Montando o Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJipPuA65BWJ",
        "outputId": "2f567e11-60da-417e-fe6f-e8f7ce120bb2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVGi-tqY6OBC"
      },
      "source": [
        "### Importando os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "QyqFhTL65jcY",
        "outputId": "59a0bb3b-7948-41ef-950a-3cbf99674c97"
      },
      "source": [
        "# Importa aquivo de texto (.txt)\n",
        "import pandas as pd\n",
        "#df_original = pd.read_csv('/content/drive/MyDrive/JOB-Martins/DadosPrevisaoDemanda.csv',sep=';', decimal=',', engine='python')\n",
        "df_original = pd.read_csv('/content/drive/MyDrive/JOB-Martins/Previsão de Demanda/PrevisaoDemandaFull.csv',sep=';')\n",
        "\n",
        "df_original['PrimeiraDataSemana'] = pd.to_datetime(df_original['PrimeiraDataSemana'])\n",
        "df_original['UltimaDataSemana'] = pd.to_datetime(df_original['UltimaDataSemana'])\n",
        "df_original = df_original.sort_values(by=['PrimeiraDataSemana'])\n",
        "df_original.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SiglaUF</th>\n",
              "      <th>DsCategoria</th>\n",
              "      <th>DsSubCategoria</th>\n",
              "      <th>CdGrupoProdutoSimilar</th>\n",
              "      <th>DsGrupoProdutoSimilar</th>\n",
              "      <th>DsMarca</th>\n",
              "      <th>DsDivisaoFornecedor</th>\n",
              "      <th>ano</th>\n",
              "      <th>PrimeiraDataSemana</th>\n",
              "      <th>UltimaDataSemana</th>\n",
              "      <th>SemanaAno</th>\n",
              "      <th>QtdeVendida</th>\n",
              "      <th>VlTotalBruto</th>\n",
              "      <th>VlTotalLiquido</th>\n",
              "      <th>VlReceitaLiquida</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>350869</th>\n",
              "      <td>BA</td>\n",
              "      <td>ISQUEIRO/FOSFORO/ACENDEDOR</td>\n",
              "      <td>ISQUEIRO</td>\n",
              "      <td>1001857.0</td>\n",
              "      <td>ISQ.BIC MAXI 12X1</td>\n",
              "      <td>Bic</td>\n",
              "      <td>BIC BRASIL S/A</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>2020-07-26</td>\n",
              "      <td>2020-08-01</td>\n",
              "      <td>31.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>33.61</td>\n",
              "      <td>31.73</td>\n",
              "      <td>25.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348816</th>\n",
              "      <td>BA</td>\n",
              "      <td>VIDRO</td>\n",
              "      <td>PRATO DE VIDRO</td>\n",
              "      <td>1000659.0</td>\n",
              "      <td>PRATO DX.ASTRAL FD.0800 24X1</td>\n",
              "      <td>Marinex</td>\n",
              "      <td>NADIR FIGUEIREDO IND E COM S/A</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>2020-07-26</td>\n",
              "      <td>2020-08-01</td>\n",
              "      <td>31.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>78.46</td>\n",
              "      <td>74.69</td>\n",
              "      <td>60.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348865</th>\n",
              "      <td>BA</td>\n",
              "      <td>SANDALIA</td>\n",
              "      <td>SLIM</td>\n",
              "      <td>1236.0</td>\n",
              "      <td>SAND.HAV.SLIM ORGANIC</td>\n",
              "      <td>Havaianas</td>\n",
              "      <td>ALPARGATAS S/A</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>2020-07-26</td>\n",
              "      <td>2020-08-01</td>\n",
              "      <td>31.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>121.08</td>\n",
              "      <td>117.42</td>\n",
              "      <td>87.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348866</th>\n",
              "      <td>BA</td>\n",
              "      <td>INSETICIDA/REPELENTE</td>\n",
              "      <td>INSETICIDA AEROSOL</td>\n",
              "      <td>697.0</td>\n",
              "      <td>INSET.AER. MULTI SBP</td>\n",
              "      <td>SBP</td>\n",
              "      <td>RECKITT BENCKISER - INSETICIDAS</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>2020-07-26</td>\n",
              "      <td>2020-08-01</td>\n",
              "      <td>31.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>37.41</td>\n",
              "      <td>34.98</td>\n",
              "      <td>28.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348867</th>\n",
              "      <td>BA</td>\n",
              "      <td>INSETICIDA/REPELENTE</td>\n",
              "      <td>INSETICIDA AEROSOL</td>\n",
              "      <td>697.0</td>\n",
              "      <td>INSET.AER. MULTI SBP</td>\n",
              "      <td>SBP</td>\n",
              "      <td>RECKITT BENCKISER - INSETICIDAS</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>2020-07-26</td>\n",
              "      <td>2020-08-01</td>\n",
              "      <td>31.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>292.00</td>\n",
              "      <td>268.64</td>\n",
              "      <td>215.70</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       SiglaUF                 DsCategoria  ... VlTotalLiquido  VlReceitaLiquida\n",
              "350869      BA  ISQUEIRO/FOSFORO/ACENDEDOR  ...          31.73             25.76\n",
              "348816      BA                       VIDRO  ...          74.69             60.61\n",
              "348865      BA                    SANDALIA  ...         117.42             87.39\n",
              "348866      BA        INSETICIDA/REPELENTE  ...          34.98             28.39\n",
              "348867      BA        INSETICIDA/REPELENTE  ...         268.64            215.70\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKsqUsBC6XGN"
      },
      "source": [
        "### Verificar distribuidores e pedidos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "NmYiIPxl5jaC",
        "outputId": "3a884149-de4b-4253-a6e8-a4f599ff2fd6"
      },
      "source": [
        "dat_aux = pd.DataFrame(df_original.groupby(['DsDivisaoFornecedor'])['CdGrupoProdutoSimilar'].unique())\n",
        "dat_aux"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CdGrupoProdutoSimilar</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DsDivisaoFornecedor</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3M DO BRASIL LTDA</th>\n",
              "      <td>[819.0, 822.0, 803.0, 818.0, 87.0, 97147.0, 97...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3M DO BRASIL LTDA - DIV. FITAS</th>\n",
              "      <td>[408214.0, 400433.0, 400434.0, 400435.0, 40667...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3M DO BRASIL LTDA - EPI</th>\n",
              "      <td>[9000524.0, 9000527.0, 9000531.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3M DO BRASIL LTDA - LIMPEZA</th>\n",
              "      <td>[1702339.0, 1704060.0, 201808.0, 200755.0, 170...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3M DO BRASIL LTDA LIMPEZA &amp;&amp;MP</th>\n",
              "      <td>[1702629.0, 1701347.0, 1701261.0, 1701262.0, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WHIRLPOOL S.A - L.BCA</th>\n",
              "      <td>[1062.0, 636.0, 2213230.0, 1809.0, 191.0, 2217...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WORLD COM IND MED VET COSM LIM</th>\n",
              "      <td>[701869.0, 701762.0, 701763.0, 701761.0, 70176...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WOW INDUSTRIA E COMERCIO LTDA</th>\n",
              "      <td>[1600609.0, 1600715.0, 1600716.0, 4301475.0, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XERYUS IMP.E DIST.ARTIGOS VEST.LTDA</th>\n",
              "      <td>[804057.0, 804061.0, 804051.0, 804291.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ZOETIS IND.PROD.VETERINARIOS LTDA</th>\n",
              "      <td>[702020.0, 700132.0, 703310.0, 701445.0, 70039...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>473 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                 CdGrupoProdutoSimilar\n",
              "DsDivisaoFornecedor                                                                   \n",
              "3M DO BRASIL LTDA                    [819.0, 822.0, 803.0, 818.0, 87.0, 97147.0, 97...\n",
              "3M DO BRASIL LTDA - DIV. FITAS       [408214.0, 400433.0, 400434.0, 400435.0, 40667...\n",
              "3M DO BRASIL LTDA - EPI                              [9000524.0, 9000527.0, 9000531.0]\n",
              "3M DO BRASIL LTDA - LIMPEZA          [1702339.0, 1704060.0, 201808.0, 200755.0, 170...\n",
              "3M DO BRASIL LTDA LIMPEZA &&MP       [1702629.0, 1701347.0, 1701261.0, 1701262.0, 1...\n",
              "...                                                                                ...\n",
              "WHIRLPOOL S.A - L.BCA                [1062.0, 636.0, 2213230.0, 1809.0, 191.0, 2217...\n",
              "WORLD COM IND MED VET COSM LIM       [701869.0, 701762.0, 701763.0, 701761.0, 70176...\n",
              "WOW INDUSTRIA E COMERCIO LTDA        [1600609.0, 1600715.0, 1600716.0, 4301475.0, 1...\n",
              "XERYUS IMP.E DIST.ARTIGOS VEST.LTDA           [804057.0, 804061.0, 804051.0, 804291.0]\n",
              "ZOETIS IND.PROD.VETERINARIOS LTDA    [702020.0, 700132.0, 703310.0, 701445.0, 70039...\n",
              "\n",
              "[473 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MinJtVge7FsY"
      },
      "source": [
        "### Agora importar as bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEJMhWYD5jU6"
      },
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "import pmdarima as pm\n",
        "#from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.metrics import r2_score\n",
        "from croston import croston\n",
        "from prophet import Prophet\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import pyEDM\n",
        "from darts import TimeSeries\n",
        "from darts.metrics import mape\n",
        "from darts.models import (\n",
        "    NaiveSeasonal,\n",
        "    NaiveDrift,\n",
        "    ExponentialSmoothing,\n",
        "    ARIMA,\n",
        "    AutoARIMA,\n",
        "    RegressionEnsembleModel,\n",
        "    RegressionModel,\n",
        "    Theta,\n",
        "    FFT\n",
        ")\n",
        "import skedm as edm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlEqX-Ik5jSF"
      },
      "source": [
        "def smape(a, f):\n",
        "    return 1/len(a) * np.sum(2 * np.abs(f-a) / (np.abs(a) + np.abs(f))*100)\n",
        "\n",
        "def MAPE(Y_actual,Y_Predicted):\n",
        "    mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100\n",
        "    return mape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n4jXiHG8aZs"
      },
      "source": [
        "### Se na série temporal exiatir algum feriado, podemos adicioná-lo no modelo prophet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "jOCSkVPN5jPU",
        "outputId": "7a11475c-fccf-415d-9e14-663014010ca9"
      },
      "source": [
        "# Feriado de independencia\n",
        "independencia = pd.DataFrame({\n",
        "  'holiday': 'independencia',\n",
        "  'ds': pd.to_datetime(['2020-09-13']),\n",
        "  'lower_window': 0,\n",
        "  'upper_window': 1,\n",
        "})\n",
        "independencia"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>holiday</th>\n",
              "      <th>ds</th>\n",
              "      <th>lower_window</th>\n",
              "      <th>upper_window</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>independencia</td>\n",
              "      <td>2020-09-13</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         holiday         ds  lower_window  upper_window\n",
              "0  independencia 2020-09-13             0             1"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ERQR1mvBja"
      },
      "source": [
        "### Extraindo as Séries temporais:\n",
        "### Com outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYGGB9POvBW5"
      },
      "source": [
        "df_2 = pd.DataFrame()\n",
        "\n",
        "total_fornecedor = df_original.DsDivisaoFornecedor.nunique()\n",
        "num_de_semanas = df_original.SemanaAno.nunique()\n",
        "\n",
        "for j in range(0, total_fornecedor):\n",
        "   filter1 = df_original[\"DsDivisaoFornecedor\"]==dat_aux.index[j]\n",
        "   df_0 = df_original[filter1]\n",
        "   df = df_0.groupby(['SemanaAno'])[['VlTotalLiquido']].sum()\n",
        "   we = pd.DataFrame(list(range(1,num_de_semanas,1)))\n",
        "   we.columns=['SemanaAno']\n",
        "   we.index=we.SemanaAno\n",
        "   df = pd.concat([df, we], axis=1)\n",
        "   df.columns=[dat_aux.index[j],'SemanaAno']\n",
        "   #df.index = pd.DataFrame(pd.date_range('2020-01-05', freq='7D', periods=num_de_semanas+1), columns=['date']).iloc[:,0]\n",
        "   df = pd.DataFrame(df).fillna(0)\n",
        "   mediana=df.median()\n",
        "\n",
        "   df = df.drop(['SemanaAno'],axis=1)\n",
        "   \n",
        "   #if [mediana==0]:\n",
        "   #  df.columns = [dat_aux.index[j]]\n",
        "   #  #df_2[dat_aux.index[j]]=df\n",
        "   #  df_2 = pd.concat([df_2,df],axis=1)\n",
        "   #else:\n",
        "   # Q1 = df.quantile(0.25)\n",
        "   #  Q3 = df.quantile(0.75)\n",
        "   ##  IQR = Q3 - Q1\n",
        "   #  df[((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR)))] = mediana\n",
        "   #  df.columns = [dat_aux.index[j]]\n",
        "    # df_2 = pd.concat([df_2,df],axis=1)\n",
        "   df_2 = pd.concat([df_2,df],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeMdQMUHrceJ"
      },
      "source": [
        "### Sem Outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8YMCJf1rfQo"
      },
      "source": [
        "df_2_sem_out = pd.DataFrame()\n",
        "\n",
        "total_fornecedor = df_original.DsDivisaoFornecedor.nunique()\n",
        "num_de_semanas = df_original.SemanaAno.nunique()\n",
        "\n",
        "for j in range(0, total_fornecedor):\n",
        "   filter1 = df_original[\"DsDivisaoFornecedor\"]==dat_aux.index[j]\n",
        "   df_0 = df_original[filter1]\n",
        "   df = df_0.groupby(['SemanaAno'])[['VlTotalLiquido']].sum()\n",
        "   we = pd.DataFrame(list(range(1,num_de_semanas,1)))\n",
        "   we.columns=['SemanaAno']\n",
        "   we.index=we.SemanaAno\n",
        "   df = pd.concat([df, we], axis=1)\n",
        "   df.columns=[dat_aux.index[j],'SemanaAno']\n",
        "   #df.index = pd.DataFrame(pd.date_range('2020-01-05', freq='7D', periods=num_de_semanas+1), columns=['date']).iloc[:,0]\n",
        "   df = pd.DataFrame(df).fillna(0)\n",
        "   mediana=df.median()[0]\n",
        "\n",
        "   df = df.drop(['SemanaAno'],axis=1)\n",
        "   \n",
        "   Q1 = df.quantile(0.25)\n",
        "   Q3 = df.quantile(0.75)\n",
        "   IQR = Q3 - Q1\n",
        "   df[((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR)))] = mediana\n",
        "   df_2_sem_out = pd.concat([df_2_sem_out,df],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2UcoX6FupTz"
      },
      "source": [
        "### Calculo ADI & CV2 e categorização das demandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1llRyLMfupH3",
        "outputId": "b45df816-6897-4ca3-d54e-a20f68aaee32"
      },
      "source": [
        "ADI={}\n",
        "CV2={}\n",
        "\n",
        "\n",
        "for i in range(0, df_2.shape[1]):\n",
        "  \n",
        "  ADI[df_2.columns[i]] = 53/sum(df_2.iloc[:,i][df_2.iloc[:,i]!=0])\n",
        "  CV2[df_2.columns[i]] = (df_2.iloc[:,i].describe()[2]/df_2.iloc[:,i].mean())**2\n",
        "\n",
        "segmentos = pd.concat([pd.DataFrame.from_dict(ADI,orient='index', columns=['ADI']),\n",
        "                       pd.DataFrame.from_dict(CV2,orient='index', columns=['CV2'])],axis=1)\n",
        "cat = np.where([(segmentos.CV2 <= 0.74) & (segmentos.ADI<=2)], 'smooth', 'lumpy')\n",
        "cat = np.where([(segmentos.CV2 > 0.74) & (segmentos.ADI>2)], 'lumpy', cat)\n",
        "cat = np.where([(segmentos.CV2 <=0.74) & (segmentos.ADI>2)], 'intermitten', cat)\n",
        "cat = np.where([(segmentos.CV2 >0.74) & (segmentos.ADI<=2)], 'erratic', cat)\n",
        "cat = np.transpose(pd.DataFrame(cat))\n",
        "cat.index = segmentos.index\n",
        "cat.columns=['cat']\n",
        "cat_croston = cat[(cat.cat=='lumpy')|(cat.cat=='intermitten')]\n",
        "cat_rest = cat[(cat.cat!='lumpy') & (cat.cat!='intermitten')]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in double_scalars\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in double_scalars\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsflmUXn23Jw"
      },
      "source": [
        "### Escolhendo distribuidores apenas 'smooth' e 'erraticos'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "veZ6DpQVupBn",
        "outputId": "2e1ec2f5-7fce-419d-8ffb-0baa0ea893f1"
      },
      "source": [
        "filter = df_original[\"DsDivisaoFornecedor\"].isin(list(cat_rest.index))\n",
        "\n",
        "df_original1 = df_original[filter]\n",
        "\n",
        "dat_aux1 = pd.DataFrame(df_original1.groupby(['DsDivisaoFornecedor'])['CdGrupoProdutoSimilar'].unique())\n",
        "dat_aux1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CdGrupoProdutoSimilar</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DsDivisaoFornecedor</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3M DO BRASIL LTDA</th>\n",
              "      <td>[819.0, 822.0, 803.0, 818.0, 87.0, 97147.0, 97...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3M DO BRASIL LTDA - DIV. FITAS</th>\n",
              "      <td>[408214.0, 400433.0, 400434.0, 400435.0, 40667...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3M DO BRASIL LTDA - LIMPEZA</th>\n",
              "      <td>[1702339.0, 1704060.0, 201808.0, 200755.0, 170...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3M DO BRASIL LTDA LIMPEZA &amp;&amp;MP</th>\n",
              "      <td>[1702629.0, 1701347.0, 1701261.0, 1701262.0, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3M DO BRASIL LTDA-ESCOLAR</th>\n",
              "      <td>[801934.0, 801936.0, 800885.0, 800886.0, 80193...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WE DO COM.PROD.ELETRONICOS EIRELI</th>\n",
              "      <td>[3303237.0, 2309845.0, 2309443.0, 2309994.0, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WHIRLPOOL S.A - L.BCA</th>\n",
              "      <td>[1062.0, 636.0, 2213230.0, 1809.0, 191.0, 2217...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WORLD COM IND MED VET COSM LIM</th>\n",
              "      <td>[701869.0, 701762.0, 701763.0, 701761.0, 70176...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WOW INDUSTRIA E COMERCIO LTDA</th>\n",
              "      <td>[1600609.0, 1600715.0, 1600716.0, 4301475.0, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ZOETIS IND.PROD.VETERINARIOS LTDA</th>\n",
              "      <td>[702020.0, 700132.0, 703310.0, 701445.0, 70039...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>293 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                               CdGrupoProdutoSimilar\n",
              "DsDivisaoFornecedor                                                                 \n",
              "3M DO BRASIL LTDA                  [819.0, 822.0, 803.0, 818.0, 87.0, 97147.0, 97...\n",
              "3M DO BRASIL LTDA - DIV. FITAS     [408214.0, 400433.0, 400434.0, 400435.0, 40667...\n",
              "3M DO BRASIL LTDA - LIMPEZA        [1702339.0, 1704060.0, 201808.0, 200755.0, 170...\n",
              "3M DO BRASIL LTDA LIMPEZA &&MP     [1702629.0, 1701347.0, 1701261.0, 1701262.0, 1...\n",
              "3M DO BRASIL LTDA-ESCOLAR          [801934.0, 801936.0, 800885.0, 800886.0, 80193...\n",
              "...                                                                              ...\n",
              "WE DO COM.PROD.ELETRONICOS EIRELI  [3303237.0, 2309845.0, 2309443.0, 2309994.0, 3...\n",
              "WHIRLPOOL S.A - L.BCA              [1062.0, 636.0, 2213230.0, 1809.0, 191.0, 2217...\n",
              "WORLD COM IND MED VET COSM LIM     [701869.0, 701762.0, 701763.0, 701761.0, 70176...\n",
              "WOW INDUSTRIA E COMERCIO LTDA      [1600609.0, 1600715.0, 1600716.0, 4301475.0, 1...\n",
              "ZOETIS IND.PROD.VETERINARIOS LTDA  [702020.0, 700132.0, 703310.0, 701445.0, 70039...\n",
              "\n",
              "[293 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxxIR-0Auo6o"
      },
      "source": [
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErahlFlWuoj1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftVmKIusg_Sh"
      },
      "source": [
        "### Agora o algoritimo para ajustar os modelos. (=~ 1 hora para 293 fornecedores)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr49xhcU5jFz"
      },
      "source": [
        "par={}\n",
        "mape_arima={}\n",
        "par_croston={}\n",
        "mape_croston={}\n",
        "mape_prophet={}\n",
        "mape_EDM = {}\n",
        "mape_naive = {}\n",
        "mape_Theta = {}\n",
        "mape_FFT = {}\n",
        "\n",
        "smape_arima={}\n",
        "smape_croston={}\n",
        "smape_prophet={}\n",
        "smape_EDM = {}\n",
        "smape_naive = {}\n",
        "smape_Theta = {}\n",
        "smape_FFT = {}\n",
        "\n",
        "pred_arima ={}\n",
        "pred_croston = {}\n",
        "pred_prophet = {}\n",
        "pred_EDM = {}\n",
        "pred_Naive ={}\n",
        "pred_Theta = {}\n",
        "pred_FFT = {}\n",
        "\n",
        "corte_serie=1\n",
        "\n",
        "\n",
        "for j in tqdm (range(0, df_original1.DsDivisaoFornecedor.nunique()),desc=\"Processando Fornecedor\", ascii=False, ncols=75):\n",
        "#for j in tqdm (range(0, 2),desc=\"Processando Fornecedor\", ascii=False, ncols=75): \n",
        "    try:\n",
        "         \n",
        "   # Selecionando os dados para ajustar os modelos\n",
        "         filter1 = df_original1[\"DsDivisaoFornecedor\"]==dat_aux1.index[j]\n",
        "         #filter1 = df_original[\"DsDivisaoFornecedor\"]=='NESTLE BRASIL - GAROTO'\n",
        "         df_0 = df_original1[filter1]\n",
        "         df = df_0.groupby(['SemanaAno'])[['VlTotalBruto']].sum()\n",
        "         we = pd.DataFrame(list(range(1,num_de_semanas,1)))\n",
        "         we.columns=['Semanas']\n",
        "         we.index=we.Semanas\n",
        "         df = pd.concat([df, we], axis=1)\n",
        "         df=df['VlTotalBruto']\n",
        "         df.index = pd.DataFrame(pd.date_range('2020-01-05', freq='7D', periods=num_de_semanas), columns=['date']).iloc[:,0]\n",
        "         df = pd.DataFrame(df).fillna(0)\n",
        "         \n",
        "\n",
        "      # Substituindo outliers\n",
        "         mediana=df.VlTotalBruto.median()\n",
        "         Q1 = df.quantile(0.25)\n",
        "         Q3 = df.quantile(0.75)\n",
        "         IQR = Q3 - Q1\n",
        "         df[((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR)))] = mediana\n",
        "          \n",
        "\n",
        "\n",
        "      # Sets de treino e teste\n",
        "         #train_len = int(df.shape[0] * corte_serie)\n",
        "         #train_len = int(corte_serie)\n",
        "         #train_data, test_data = df[:train_len], df[train_len:]\n",
        "         test_data = train_data = df\n",
        "\n",
        "      # Ajustando AutoArima\n",
        "         stepwise_fit = pm.auto_arima(train_data, start_p=1, start_q=1, max_p=3, max_q=3, m=12,\n",
        "                             start_P=0, seasonal=True, d=1, D=1, trace=False,\n",
        "                             error_action='ignore',  # don't want to know if an order does not work\n",
        "                             suppress_warnings=True,  # don't want convergence warnings\n",
        "                             stepwise=True)  # set to stepwise\n",
        "   \n",
        "      # Armazenando resultados AutoArima\n",
        "         sentence = [str(dat_aux1.index[j])]\n",
        "         sentence = '-'.join(sentence)\n",
        "         aux = stepwise_fit.get_params()\n",
        "         par[sentence]=aux\n",
        "         #forecasts = stepwise_fit.predict(test_data.shape[0])\n",
        "         forecasts = stepwise_fit.predict_in_sample(test_data.shape[0])\n",
        "         #mape_arima[sentence]=mean_absolute_percentage_error(list(list(test_data.VlTotalBruto)), list(forecasts))\n",
        "         mape_arima[sentence]=MAPE(test_data.VlTotalBruto, forecasts)\n",
        "         pred_arima[sentence]=forecasts\n",
        "         smape_arima [sentence] = smape(test_data.VlTotalBruto, forecasts)\n",
        "         \n",
        "\n",
        "  \n",
        "      # Ajustando modelo Croston\n",
        "         croston_pred = croston.fit_croston(df+0.000001,test_data.shape[0],'original')\n",
        "\n",
        "      # Armazenando resultados Croston\n",
        "         #par_croston[sentence] = croston_pred\n",
        "         #mape_croston[sentence] = mean_absolute_percentage_error(list(test_data.VlTotalBruto), croston_pred['croston_forecast'])\n",
        "         #mape_croston[sentence] = MAPE(test_data.VlTotalBruto, croston_pred['croston_forecast'])\n",
        "         #smape_croston[sentence] = smape(test_data.VlTotalBruto, pd.DataFrame(croston_pred['croston_forecast']).iloc[:,0])\n",
        "         #pred_croston[sentence] = pd.DataFrame(croston_pred['croston_forecast']).iloc[:,0]\n",
        "         \n",
        "      # O modelo profet só funciona com duas colunas\n",
        "         y = df.reset_index(drop=False)\n",
        "         y.columns = ['ds', 'y']\n",
        "         #y['floor'] = 0\n",
        "         #y['cap'] = 200000\n",
        "         train = y.iloc[:train_data.shape[0],:]\n",
        "         test = y.iloc[train_data.shape[0]:,:]\n",
        "         test=train\n",
        "\n",
        "      # Ajustando modelo Prophet\n",
        "         m = Prophet(weekly_seasonality = True,\n",
        "                     yearly_seasonality = False,\n",
        "                     daily_seasonality = False)\n",
        "         m.fit(train,verbose=0)\n",
        "         future = m.make_future_dataframe(periods=len(test))\n",
        "         future['ds'] = pd.to_datetime(future['ds']).dt.date\n",
        "         forecast = m.predict(future)\n",
        "         one=test['y']\n",
        "         twi=list(forecast.loc[:test.shape[0]-1,'yhat'])\n",
        "         #mape_prophet[sentence] = mean_absolute_percentage_error(list(one),list(twi))# Assimetrico\n",
        "         mape_prophet[sentence] = MAPE(one,twi)# Assimetrico\n",
        "         smape_prophet[sentence] = smape(one, twi) # simmetrico\n",
        "         pred_prophet[sentence] = twi\n",
        "         \n",
        "\n",
        "\n",
        "         # O EDM agora, preparando o imput\n",
        "         y = y[[\"ds\",\"y\"]]\n",
        "         y.columns = [\"Time\", \"Revenue\"]\n",
        "         y.Time = list(range(1,df.shape[0]+1,1))\n",
        "         \n",
        "         #aaa=pyEDM.EmbedDimension(dataFrame = y, lib=\"1 53\", pred=\"1 53\", columns=\"Revenue\",target = \"Revenue\",showPlot=False)\n",
        "         #maxE = aaa.rho.max()\n",
        "         #selectedE=aaa[aaa.rho==maxE]['E']\n",
        "         #intervalo = pyEDM.PredictInterval(dataFrame = y,lib=\"1 53\", pred=\"1 53\", columns=\"Revenue\", E=3, showPlot=False)\n",
        "         #maxpred = intervalo.rho.max()\n",
        "         #selectedpred=intervalo[intervalo.rho==maxpred]['Tp']\n",
        "         simplex_pred = pyEDM.Simplex(dataFrame = y,lib=\"1 53\", pred=\"1 53\", E=3, columns=\"Revenue\",Tp=0,tau=-1)\n",
        "#print(pyEDM.ComputeError(simplex_pred['Observations'],simplex_pred['Predictions']))\n",
        "         simplex_pred = simplex_pred[['Observations','Predictions']].dropna()\n",
        "         pred_EDM[sentence] = list(simplex_pred['Predictions'])\n",
        "         #mape_EDM[sentence] = mean_absolute_percentage_error(list(simplex_pred['Observations']), list(simplex_pred['Predictions']))\n",
        "         mape_EDM[sentence] = MAPE(simplex_pred['Observations'], simplex_pred['Predictions'])\n",
        "         smape_EDM[sentence] = smape(simplex_pred['Observations'], simplex_pred['Predictions'])\n",
        "         \n",
        "         # PAra Naive, exponential smothing, theta, FFT\n",
        "         series = TimeSeries.from_dataframe(df.reset_index(), 'date', 'VlTotalBruto')\n",
        "\n",
        "         #train2, val = series.split_after(1)\n",
        "         train2 = val = series\n",
        "\n",
        "         #trains2=val=series\n",
        "         model1 = NaiveDrift()\n",
        "         model1.fit(train2+0.00000001)\n",
        "         pred_val1 = model1.predict(len(val))\n",
        "         pred_Naive[sentence] = pd.DataFrame(pred_val1.values()).iloc[:,0]\n",
        "         #mape_naive[sentence] = mean_absolute_percentage_error(val.values(),pred_val1.values())\n",
        "         mape_naive[sentence] = MAPE(val.values(),pred_val1.values())\n",
        "         smape_naive[sentence] = str(smape(val.values(),pred_val1.values()))\n",
        "         \n",
        "         #model2 = ExponentialSmoothing()\n",
        "         #model2.fit(train2+0.00000001)\n",
        "         #pred_val2 = model2.predict(len(val))\n",
        "         #pred_ExponentialSmoothing[sentence] = pd.DataFrame(pred_val2.values()).iloc[:,0]\n",
        "         #mape_ExponentialSmoothing[sentence] = mean_absolute_percentage_error(val.values(),pred_val2.values())\n",
        "         #smape_ExponentialSmoothing[sentence] = str(smape(val.values(),pred_val2.values()))\n",
        "         \n",
        "\n",
        "         model3 = Theta()\n",
        "         model3.fit(train2+0.00000001)\n",
        "         pred_val3 = model3.predict(len(val))\n",
        "         pred_Theta[sentence] = pd.DataFrame(pred_val3.values()).iloc[:,0]\n",
        "         #mape_Theta[sentence] = mean_absolute_percentage_error(val.values(),pred_val3.values())\n",
        "         mape_Theta[sentence] = MAPE(val.values(),pred_val3.values())\n",
        "         smape_Theta[sentence] = str(smape(val.values(),pred_val3.values()))\n",
        "         \n",
        "\n",
        "         model4 = FFT(trend='poly',trend_poly_degree = 2)\n",
        "         model4.fit(train2+0.00000001)\n",
        "         pred_val4 = model4.predict(len(val))\n",
        "         pred_FFT[sentence] = pd.DataFrame(pred_val4.values()).iloc[:,0]\n",
        "         #mape_FFT[sentence] = mean_absolute_percentage_error(val.values(),pred_val4.values())\n",
        "         mape_FFT[sentence] = MAPE(val.values(),pred_val4.values())\n",
        "         smape_FFT[sentence] = str(smape(val.values(),pred_val4.values()))\n",
        "\n",
        "    except:\n",
        "     pass  \n",
        "\n",
        "resultado_final_mape= pd.concat([pd.DataFrame.from_dict(mape_arima,orient='index'),\n",
        "           #pd.DataFrame.from_dict(mape_croston,orient='index'),\n",
        "           pd.DataFrame.from_dict(mape_prophet,orient='index'),\n",
        "           pd.DataFrame.from_dict(mape_EDM,orient='index'),\n",
        "           pd.DataFrame.from_dict(mape_naive,orient='index'),\n",
        "           #pd.DataFrame.from_dict(mape_ExponentialSmoothing,orient='index'),\n",
        "           pd.DataFrame.from_dict(mape_Theta,orient='index'),\n",
        "           pd.DataFrame.from_dict(mape_FFT,orient='index')   \n",
        "           ], axis=1)\n",
        "resultado_final_mape.columns=[\"arima\", \"prophet\",\"EDM\",\"naive\",\"theta\",\"fft\"]\n",
        "\n",
        "resultado_final_smape= pd.concat([pd.DataFrame.from_dict(smape_arima,orient='index'),\n",
        "           #pd.DataFrame.from_dict(smape_croston,orient='index'),\n",
        "           pd.DataFrame.from_dict(smape_prophet,orient='index'),\n",
        "           pd.DataFrame.from_dict(smape_EDM,orient='index'),\n",
        "           pd.DataFrame.from_dict(smape_naive,orient='index'),\n",
        "           #pd.DataFrame.from_dict(smape_ExponentialSmoothing,orient='index'),\n",
        "           pd.DataFrame.from_dict(smape_Theta,orient='index'),\n",
        "           pd.DataFrame.from_dict(smape_FFT,orient='index')   \n",
        "           ], axis=1)\n",
        "resultado_final_smape.columns=[\"arima\", \"prophet\",\"EDM\",\"naive\",\"theta\",\"fft\"]\n",
        "\n",
        "predicoes = pd.concat([pd.DataFrame.from_dict(pred_arima,orient='index'),\n",
        "           #pd.DataFrame.from_dict(pred_croston,orient='index'),\n",
        "           pd.DataFrame.from_dict(pred_prophet,orient='index'),\n",
        "           pd.DataFrame.from_dict(pred_EDM,orient='index'),\n",
        "           pd.DataFrame.from_dict(pred_Naive,orient='index'),\n",
        "           #pd.DataFrame.from_dict(pred_ExponentialSmoothing,orient='index'),\n",
        "           pd.DataFrame.from_dict(pred_Theta,orient='index'),\n",
        "           pd.DataFrame.from_dict(pred_FFT,orient='index')   \n",
        "           ], axis=0)\n",
        "predicoes['modelos']= [\"arima\"]*len(pred_arima) + [\"prophet\"]*len(pred_prophet) + [\"EDM\"]*len(pred_EDM) + ['naive']*len(pred_Naive) + ['theta']*len(pred_Theta) + ['fft']*len(pred_FFT)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L1SEV6n-Sqw"
      },
      "source": [
        "### Observados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q2jvhPtqpp2"
      },
      "source": [
        "filter = df_2.columns.isin(list(cat_rest.index))\n",
        "df3 = df_2.iloc[:,filter]\n",
        "df3t = df3.T\n",
        "df3t.columns = ['OBS_C_OUT' + str(i) for i in range (1, 54)]\n",
        "df3tm = df3t.reset_index().melt(id_vars='index')\n",
        "df3tm.columns = ['fornecedor','semana','valor']\n",
        "df3tm['modelo'] = ['obs_com_out']*df3tm.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paH8nK-VtZtS"
      },
      "source": [
        "filter = df_2_sem_out.columns.isin(list(cat_rest.index))\n",
        "df3_sem_out = df_2_sem_out.iloc[:,filter]\n",
        "df3_sem_outt = df3_sem_out.T\n",
        "df3_sem_outt.columns = ['OBS_S_OUT' + str(i) for i in range (1, 54)]\n",
        "df3_sem_outtm = df3_sem_outt.reset_index().melt(id_vars='index')\n",
        "df3_sem_outtm.columns = ['fornecedor','semana','valor']\n",
        "df3_sem_outtm['modelo'] = ['obs_sem_out']*df3_sem_outtm.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDSsiQBirFwJ"
      },
      "source": [
        "### Predicoes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIVSuKa9utXM"
      },
      "source": [
        "#df_empty = pd.DataFrame(index=predicoes.index, columns=range(0,50))\n",
        "#predicoes_4 = pd.concat([df_empty,predicoes],axis=1)\n",
        "#predicoes_4.columns = range(1,56)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk815wzp0AhL"
      },
      "source": [
        "arima = predicoes[predicoes.modelos==predicoes.modelos.unique()[0]].iloc[:,:53].reset_index().melt(id_vars='index')\n",
        "arima.columns = ['fornecedor','semana','valor']\n",
        "arima['modelo'] = ['arima']*arima.shape[0]\n",
        "arima['mape'] = pd.concat([resultado_final_mape.reset_index()]*53, ignore_index=True).iloc[:,1]\n",
        "arima['smape'] = pd.concat([resultado_final_smape.reset_index()]*53, ignore_index=True).iloc[:,1]\n",
        "\n",
        "propheta = predicoes[predicoes.modelos==predicoes.modelos.unique()[1]].iloc[:,:53].reset_index().melt(id_vars='index')\n",
        "propheta.columns = ['fornecedor','semana','valor']\n",
        "propheta['modelo'] = ['propheta']*propheta.shape[0]\n",
        "propheta['mape'] = pd.concat([resultado_final_mape.reset_index()]*53, ignore_index=True).iloc[:,2]\n",
        "propheta['smape'] = pd.concat([resultado_final_smape.reset_index()]*53, ignore_index=True).iloc[:,2]\n",
        "\n",
        "EDM = predicoes[predicoes.modelos==predicoes.modelos.unique()[2]].iloc[:,:53].reset_index().melt(id_vars='index')\n",
        "EDM.columns = ['fornecedor','semana','valor']\n",
        "EDM['modelo'] = ['EDM']*EDM.shape[0]\n",
        "EDM['mape'] = pd.concat([resultado_final_mape.reset_index()]*53, ignore_index=True).iloc[:,3]\n",
        "EDM['smape'] = pd.concat([resultado_final_smape.reset_index()]*53, ignore_index=True).iloc[:,3]\n",
        "\n",
        "naives = predicoes[predicoes.modelos==predicoes.modelos.unique()[3]].iloc[:,:53].reset_index().melt(id_vars='index')\n",
        "naives.columns = ['fornecedor','semana','valor']\n",
        "naives['modelo'] = ['naives']*naives.shape[0]\n",
        "naives['mape'] = pd.concat([resultado_final_mape.reset_index()]*53, ignore_index=True).iloc[:,4]\n",
        "naives['smape'] = pd.concat([resultado_final_smape.reset_index()]*53, ignore_index=True).iloc[:,4]\n",
        "\n",
        "thetas = predicoes[predicoes.modelos==predicoes.modelos.unique()[4]].iloc[:,:53].reset_index().melt(id_vars='index')\n",
        "thetas.columns = ['fornecedor','semana','valor']\n",
        "thetas['modelo'] = ['thetas']*thetas.shape[0]\n",
        "thetas['mape'] = pd.concat([resultado_final_mape.reset_index()]*53, ignore_index=True).iloc[:,5]\n",
        "thetas['smape'] = pd.concat([resultado_final_smape.reset_index()]*53, ignore_index=True).iloc[:,5]\n",
        "\n",
        "ffts = predicoes[predicoes.modelos==predicoes.modelos.unique()[5]].iloc[:,:53].reset_index().melt(id_vars='index')\n",
        "ffts.columns = ['fornecedor','semana','valor']\n",
        "ffts['modelo'] = ['ffts']*ffts.shape[0]\n",
        "ffts['mape'] = pd.concat([resultado_final_mape.reset_index()]*53, ignore_index=True).iloc[:,6]\n",
        "ffts['smape'] = pd.concat([resultado_final_smape.reset_index()]*53, ignore_index=True).iloc[:,6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpEv_pcS3ev_"
      },
      "source": [
        "RES_FINAL = pd.concat([df3tm, df3_sem_outtm, arima, propheta,EDM, naives, thetas, ffts],axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5G8yPxN3elR"
      },
      "source": [
        "RES_FINAL.to_excel('/content/drive/MyDrive/JOB-Martins/Previsão de Demanda/RES_FINAL.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyJ5JtqCP-Dc"
      },
      "source": [
        "### Arrumando a matrix para agrupar os resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8hyK3GhvguO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXkSGYywvgmz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1bslQai01CO"
      },
      "source": [
        "predicoes.columns = predicoes.columns.droplevel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q9eSdCNOs0Y"
      },
      "source": [
        "resul_pred = predicoes.reset_index().melt(id_vars=['index','modelos'])\n",
        "resul_pred = resul_pred.reset_index().sort_values(['index','modelos','level_0'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9BkygC3OswT"
      },
      "source": [
        "resultado_final_mape.columns = resultado_final_mape.columns.droplevel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbp5lZlgOssY"
      },
      "source": [
        "# 53 pois são 53 semanas\n",
        "erro_mape = pd.concat([resultado_final_mape.reset_index().melt(id_vars='index')]*53, ignore_index=True)\n",
        "erro_mape = erro_mape.sort_values(['index','variable'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nncjfBCkjlnR"
      },
      "source": [
        "resultado_final_smape.columns = resultado_final_smape.columns.droplevel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjIp3E7TOso1"
      },
      "source": [
        "# 53 pois são 53 semanas\n",
        "erro_smape = pd.concat([resultado_final_smape.reset_index().melt(id_vars='index')]*53, ignore_index=True)\n",
        "erro_smape = erro_smape.sort_values(['index','variable'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zOC7XuiOsch"
      },
      "source": [
        "df3t.columns = df3t.columns.droplevel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIzaTAbdgaoL"
      },
      "source": [
        "# 6 pois foram 6 ajustes de 6 modelos diferentes.\n",
        "resul_obs = pd.concat([df3t.reset_index().melt(id_vars='index').sort_values(['index','SemanaAno'])]*6, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oMMwzZhhJ-9"
      },
      "source": [
        "RES_FINAL = pd.concat([resul_obs, resul_pred,erro_mape,erro_smape],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFX3Eo0WhJxK"
      },
      "source": [
        "RES_FINAL.to_excel('/content/drive/MyDrive/JOB-Martins/Previsão de Demanda/RES_FINAL.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFfIgFUv1oF2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h67eSI1E1n3s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTYGrIiT1nvF"
      },
      "source": [
        "pd.concat([erro_mape,erro_smape.value],axis=1).to_excel('/content/drive/MyDrive/JOB-Martins/Previsão de Demanda/ERROS.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUQzzLWPhQlF"
      },
      "source": [
        "### Agora para apenas Croston"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7A9Yr8rDA94"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "HEOYGdwWDA4u",
        "outputId": "174dac57-0e0f-46f7-fbd8-2f21a6f3e7ec"
      },
      "source": [
        "filter = df_original[\"DsDivisaoFornecedor\"].isin(list(cat_croston.index))\n",
        "\n",
        "df_original2 = df_original[filter]\n",
        "\n",
        "dat_aux2 = pd.DataFrame(df_original2.groupby(['DsDivisaoFornecedor'])['CdGrupoProdutoSimilar'].unique())\n",
        "dat_aux2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CdGrupoProdutoSimilar</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DsDivisaoFornecedor</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3M DO BRASIL LTDA - EPI</th>\n",
              "      <td>[9000524.0, 9000527.0, 9000531.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ADELBRAS IND COM ADESIVOS &amp;&amp;MP</th>\n",
              "      <td>[408235.0, 408237.0, 408236.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AGIS - LOGITECH</th>\n",
              "      <td>[2309279.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AGIS COMPONENTES INFORMATICA</th>\n",
              "      <td>[2309636.0, 2309637.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AGP TECNOLOGIA INFORM.BRASIL LTDA</th>\n",
              "      <td>[2309922.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VEGA STEEL IND LTDA MARTCON &amp;&amp;MP</th>\n",
              "      <td>[1702981.0, 1702982.0, 1702979.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VIAPOL LTDA</th>\n",
              "      <td>[408446.0, 408457.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VIQUA IND. DE PLASTICOS LTDA</th>\n",
              "      <td>[4807515.0, 4807520.0, 4807505.0, 4807517.0, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WATANABE COM IMP LTDA</th>\n",
              "      <td>[1100.0, 1416.0, 9000809.0, 9000810.0, 1410.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XERYUS IMP.E DIST.ARTIGOS VEST.LTDA</th>\n",
              "      <td>[804057.0, 804061.0, 804051.0, 804291.0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>180 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                 CdGrupoProdutoSimilar\n",
              "DsDivisaoFornecedor                                                                   \n",
              "3M DO BRASIL LTDA - EPI                              [9000524.0, 9000527.0, 9000531.0]\n",
              "ADELBRAS IND COM ADESIVOS &&MP                          [408235.0, 408237.0, 408236.0]\n",
              "AGIS - LOGITECH                                                            [2309279.0]\n",
              "AGIS COMPONENTES INFORMATICA                                    [2309636.0, 2309637.0]\n",
              "AGP TECNOLOGIA INFORM.BRASIL LTDA                                          [2309922.0]\n",
              "...                                                                                ...\n",
              "VEGA STEEL IND LTDA MARTCON &&MP                     [1702981.0, 1702982.0, 1702979.0]\n",
              "VIAPOL LTDA                                                       [408446.0, 408457.0]\n",
              "VIQUA IND. DE PLASTICOS LTDA         [4807515.0, 4807520.0, 4807505.0, 4807517.0, 4...\n",
              "WATANABE COM IMP LTDA                [1100.0, 1416.0, 9000809.0, 9000810.0, 1410.0,...\n",
              "XERYUS IMP.E DIST.ARTIGOS VEST.LTDA           [804057.0, 804061.0, 804051.0, 804291.0]\n",
              "\n",
              "[180 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyc77meTDAy7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjZnivAWDAsI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2kDcBAdhVXz",
        "outputId": "1dd3ab6a-9270-4143-e6d6-aae5653e5925"
      },
      "source": [
        "par_croston={}\n",
        "mape_croston={}\n",
        "smape_croston={}\n",
        "maape_croston={}\n",
        "pred_croston = {}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "corte_serie=0.94\n",
        "#for j, i in zip(tqdm (range(0, 7),desc=\"Processando Fornecedor\", ascii=False, ncols=75), tqdm (range(0,len(dat_aux.iloc[j,0])),desc=\"Processando Id Oriduto\", ascii=False, ncols=75)):\n",
        "\n",
        "for j in tqdm (range(0, dat_aux2.shape[0]),desc=\"Processando Fornecedor\", ascii=False, ncols=75):\n",
        "  \n",
        "    #try:\n",
        "         \n",
        "   # Selecionando os dados para ajustar os modelos\n",
        "         \n",
        "         filter1 = df_original2[\"DsDivisaoFornecedor\"]==dat_aux2.index[j]\n",
        "         #filter1 = df_original[\"DsDivisaoFornecedor\"]=='NESTLE BRASIL - GAROTO'\n",
        "         df_0 = df_original2[filter1]\n",
        "         df = df_0.groupby(['SemanaAno'])[['VlTotalBruto']].sum()\n",
        "         we = pd.DataFrame(list(range(0,num_de_semanas,1)))\n",
        "         we.columns=['Semanas']\n",
        "         we.index=we.Semanas\n",
        "         df = pd.concat([df, we], axis=1)\n",
        "         df=df['VlTotalBruto']\n",
        "         df.index = pd.DataFrame(pd.date_range('2020-01-05', freq='7D', periods=num_de_semanas), columns=['date']).iloc[:,0]\n",
        "         df = pd.DataFrame(df).fillna(0)\n",
        "         \n",
        "\n",
        "         if [mediana==0]:\n",
        "           df.columns = ['VlTotalBruto']\n",
        "    \n",
        "    \n",
        "         else:\n",
        "           Q1 = df.quantile(0.25)\n",
        "           Q3 = df.quantile(0.75)\n",
        "           IQR = Q3 - Q1\n",
        "           df[((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR)))] = mediana\n",
        "           df.columns = ['VlTotalBruto']\n",
        "     \n",
        "   \n",
        "          \n",
        "\n",
        "\n",
        "      # Sets de treino e teste\n",
        "         train_len = int(df.shape[0] * corte_serie)\n",
        "         #train_len = int(corte_serie)\n",
        "         train_data, test_data = df[:train_len], df[train_len:]\n",
        "         #test_data = train_data\n",
        "         test_data = train_data = df\n",
        "\n",
        "      # Ajustando modelo Croston\n",
        "         croston_pred = croston.fit_croston(df+0.000001,test_data.shape[0],'original')\n",
        "\n",
        "      # Armazenando resultados Croston\n",
        "         par_croston[dat_aux2.index[j]] = croston_pred\n",
        "         mape_croston[dat_aux2.index[j]] = mean_absolute_percentage_error(list(test_data.VlTotalBruto), croston_pred['croston_forecast'])\n",
        "         smape_croston[dat_aux2.index[j]] = smape(test_data.VlTotalBruto, pd.DataFrame(croston_pred['croston_forecast']).iloc[:,0])\n",
        "         pred_croston[dat_aux2.index[j]] = pd.DataFrame(croston_pred['croston_forecast']).iloc[:,0]\n",
        "         \n",
        "\n",
        "      \n",
        "         \n",
        "    #except:\n",
        "     #pass  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processando Fornecedor: 100%|████████████| 180/180 [00:11<00:00, 16.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxIECrkXEiaA"
      },
      "source": [
        "pred_croston"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC1VB7YsmFxg"
      },
      "source": [
        "observado = df_2.loc[:,list(dat_aux2.index)].transpose()\n",
        "observado.columns = range(0,53)\n",
        "observado = observado.reset_index()\n",
        "observado = pd.melt(observado, id_vars =['index'])\n",
        "observado['modelo'] = ['observado']*observado.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh1Ne65inZdp"
      },
      "source": [
        "predito = pd.DataFrame.from_dict(pred_croston).T.reset_index().melt(id_vars='index')\n",
        "predito['modelo'] = ['predito']*predito.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYwDC_gwurnz"
      },
      "source": [
        "observado.to_excel('/content/drive/MyDrive/JOB-Martins/Previsão de Demanda/obs_croston.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulT9TFw4BEPv"
      },
      "source": [
        "pd.concat([observado, predito],axis=0).to_excel('/content/drive/MyDrive/JOB-Martins/Previsão de Demanda/FINAL_CROSTON.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}